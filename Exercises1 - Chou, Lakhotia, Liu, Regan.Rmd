---
title: "Exercises 1"
author: "Vincent Chou, Nikita Lakhotia, Fan Liu, Hillary Regan"
date: "August 8, 2016"
output: word_document
---

### Probability practice
#### Part A.
Here's a question a friend of mine was asked when he interviewed at Google.

Visitors to your website are asked to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories: Random Clicker (RC), and Truthful Clicker (TC). There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is 0.3.

After a trial period, you get the following survey results: 65% said Yes and 35% said No.

What fraction of people who are truthful clickers answered yes?

#### Answer:
Using the Law of Total Probability ("mixture rule"):
$$ P(Y) = P(Y|RC) * P(RC) + P(Y|TC) * P(TC) $$
where: 

- P(Y) = .65, the probability a person said Yes

- P(Y|RC) = .5 , the probability a person said Yes given they randomly clicked

- P(RC) = .3 the probability a person randomly clicked

- P(Y|TC) = ?, the probability a person said Yes given they truthfully clicked

- P(TC) = .7, the probability a person truthfully clicked

Therefore, using the formula for the Law of Total Probability: 
$$ .65 = (.5)*(.3) + P(Y|TC)*(.7) $$
Solving for P(Y|TC) gives: 

```{r}
ProbabilityTruthYes = (.65-(.5*.3))/.7
```

$$ P(Y|TC) = `r ProbabilityTruthYes` $$
The fraction of people who are truthful clickers answered yes is `r ProbabilityTruthYes`.

#### Part B.

Imagine a medical test for a disease with the following two attributes:

The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.
The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.
In the general population, incidence of the disease is reasonably rare: about 0.0025% of all people have it (or 0.000025 as a decimal probability).

Suppose someone tests positive. What is the probability that they have the disease? In light of this calculation, do you envision any problems in implementing a universal testing policy for the disease?

#### Answer:

Using Bayes' Rule: 
$$ P(A|B) = \frac{P(A)*P(B|A)}{P(B)} $$

A: person has the disease

B: test is positive

- P(A|B) = ?, the probability of having the disease given the test was positive

- P(A) = .000025, the probability of having the disease

- P(B|A) = .993, the probability that the test is positive given the person has the disease

- P(B) = ?, the probability the test is positive.

P(B) can be found by using the Law of Total Probability ("mixture rule"):
$$ P(B) = P(B|A) * P(A) + P(B|notA) * P(notA) $$
where P(B|notA) = (1-.9999), the probability that the test is positive given the person does not have the disease

$$ P(B) = (.993)*(.000025) + (1-0.9999)*(1-.000025) $$

```{r}
ProbabilityTestPositive = (.993)*(.000025) + (1-0.9999)*(1-.000025)
ProbabilityDiseasePositive = (.000025)*(.993)/ProbabilityTestPositive
```
$$ P(B) = `r ProbabilityTestPositive`$$

so, $$ P(A|B) = \frac{P(A)*P(B|A)}{P(B)} = \frac{(.000025)(.993)}{`r ProbabilityTestPositive`} = `r ProbabilityDiseasePositive` $$

The probability that a person has the disease given the test is positive is `r ProbabilityDiseasePositive`. Since the chance of this is small, I do envision problems in implementing a universal testing policy for the disease. This is not a good enough test for the disease.


### Exploratory Analysis: Green Buildings

####Overview: 
Buildings can be certified-green by two organizations, Energystar and LEED. We looked at the effects that this certification can have on rent prices. This is imperative because there are higher upfront costs to creating a green-certified building. If consumers are not willing to pay a higher rent for these buildings, there is no economic reason to build a green-certified building. A previous analysis of this data found that there is a premium on green-certified buildings. 


####Data and Model: 
Our dataset contains green-certified residential buildings and the buildings around them (to control for the economic value of the locations of these buildings) as well as numerous other variables that may affect rent (local weather, building size, etc). We used a linear regression model to try to isolate the effects of having a green certified building because the coefficients in a linear regression model show the effects of variables holding all others constant.  

####Results:

```{r echo = FALSE, results='hide', message=FALSE, warning=FALSE}
library(mosaic)
library(foreach)
GreenBuildings = read.csv('C:/Users/hilla/Desktop/R Dir/Stats part 2/greenbuildings.csv')
```

```{r echo = FALSE}
summary(GreenBuildings)
```


```{r echo=FALSE}
hist(GreenBuildings$leasing_rate, main = "Histogram of the Leasing Rate of Buildings", sub = "Figure 1")
```


The "stats guru" was correct when he said "a handful of the buildings in the data set had very low occupancy rates" which is shown by the histogram above. Yet, I do not agree with removing these buildings from consideration. There is not a guarantee that the new building will have a high occupancy rate. 

```{r echo = FALSE}

boxplot(GreenBuildings$Rent~GreenBuildings$green_rating, main="Rent of Buildings", sub = "Figure 2", xlab="Green Building or not", ylab="Rent")
```


On the x-axis, 0 represents the buildings that are not green. 1 represents buildings that have a green rating. The median rent is very similar for the two types of buildings, as is the first and third quartiles, but the rent for a green building is slightly higher. There are more outliers for non-green buildings.

```{r echo=FALSE}
par(mfrow = c(1,2))
boxplot(GreenBuildings$age~GreenBuildings$green_rating, main="Age of Buildings", sub = "Figure 3", xlab="Green Building or not", ylab="Age")
boxplot(GreenBuildings$size~GreenBuildings$green_rating, main="Size of Buildings" , sub = "Figure 4", xlab="Green Building or not", ylab="Size")
mtext('Figure 3', outer = TRUE, cex = 1.5)
```

On average, green buildings tend to be newer buildings and bigger buildings compared to non-green buildings.

```{r echo=FALSE}
model <- lm(Rent~., data=GreenBuildings)
summary(model)

```

####Conclusions: 
We could not find evidence that having a green-certified building has a significant increase in rent prices. The "Excel Guru" that performed the previous study did not account for confounders. As shown in **Figures 3 & 4**, green buildings tend to be newer buildings and bigger buildings which our model showed to be actual significant variables. This would account for the difference in medians that the "Excel Guru" identified. Our final decision would have to be to save on costs and not become green-certified.


### Bootstrapping

#### Overview: 
There is a notional $100,000 to invest in assets: US domestic equities (SPY: the S&P 500 stock index), US Treasury bonds (TLT), Investment-grade corporate bonds (LQD), Emerging-market equities (EEM), and Real estate (VNQ). We consider three portfolios, the even split, safe and aggressive portfolios. 

#### Data and Model: 
Data from daily reports of exchange-traded funds were chosen over a ten year period, including the 2008, the year the housing market crashed, to ensure that the data included both good runs and bad runs of stock-market performance. Three portfolios were considered. The first being the even split which entails 20% of the assets in each of the five ETFs above. Second, a portfolio safer than the even split, comprising investments in 'LQD','EEM','VNQ'. The allocation was 75% in 'LQD', 12.5% in 'EEM', and 12.5% in 'VNQ.' These 3 were chosen based on figure X, which shows the historical adjusted closing price for each of the five ETFs, noticing the previous trends and the stability of prices for these investments over the years. 75% was invested in 'LQD' due to the fact that 'LQD' was less affected during 2008 financial crisis, so it is a safer choice than 'EEM' and 'VNQ'. The third portfolio considered was more aggressive. It comprised of investments of 90% in 'SPY' and 10% in 'TLT'. This portfolio included exchange-traded funds that previously had higher returns, but showed a higher risk of loss in Figure X. Note that even though 'TLT' was not affected by the housing market crash at all, there were several sharp drops over the years.  Bootstrap resampling was used to estimate the 4-week (20 trading day) expected return (mean) and value at risk of each of the three portfolios at the 5% level.

#### Results: 

```{r echo=FALSE,results='hide', message=FALSE, warning=FALSE}
library(mosaic)
library(fImport)
library(foreach)
```

```{r echo=FALSE, warning=FALSE}
set.seed(10)
mystocks = c("SPY",'TLT','LQD','EEM','VNQ')
myprices = yahooSeries(mystocks, from='2005-01-01', to='2016-07-30')
head(myprices)

#helper function
YahooPricesToReturns = function(series) {
  mycols = grep('Adj.Close', colnames(series))
  closingprice = series[,mycols]
  N = nrow(closingprice)
  percentreturn = as.data.frame(closingprice[2:N,]) / as.data.frame(closingprice[1:(N-1),]) - 1
  mynames = strsplit(colnames(percentreturn), '.', fixed=TRUE)
  mynames = lapply(mynames, function(x) return(paste0(x[1], ".PctReturn")))
  colnames(percentreturn) = mynames
  as.matrix(na.omit(percentreturn))
}

#plotting
df=myprices[,c(6,12,18,24,30)]
plot(df, plot.type="single", col = 1:ncol(df))
legend("topleft", colnames(df), col=1:ncol(df), lty=1, cex=.65)

myreturns = YahooPricesToReturns(myprices)
n_days = 20
############################even split###############################
even_split = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * totalwealth
  wealthtracker = rep(0, n_days) 
  for(today in 1:n_days) {
    return.today = resample(myreturns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

head(even_split)
hist(even_split[,n_days], 25, main = "Expected Return")
mean_even_split = mean(even_split[,n_days])
abline(v=mean_even_split, lwd=4, col='blue')

# Profit/loss
hist(even_split[,n_days]- 10000, main = "Expected Profit/Loss")

# Calculate 5% value at risk
evenVaR = quantile(even_split[,n_days], 0.05) - 10000


####################################safer choice##############################
head(myreturns)
myreturns_safe=myreturns[,c(3,4,5)]   #'LQD','EEM','VNQ'
head(myreturns_safe)

safe = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.75, 0.125, 0.125)
  holdings = weights * totalwealth
  wealthtracker = rep(0, n_days) 
  for(today in 1:n_days) {
    return.today = resample(myreturns_safe, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

head(safe)
hist(safe[,n_days], 25, main = "Expected Return")
mean_safe = mean(safe[,n_days])
abline(v=mean_safe, lwd=4, col='blue')

# Profit/loss
hist(safe[,n_days]- 10000, main = "Expected Profit/Loss")

# Calculate 5% value at risk
safeVaR = quantile(safe[,n_days], 0.05) - 10000


################################aggressive##############################
myreturns_agg=myreturns[,c(1,2)]   #"SPY" and 'TLT'
head(myreturns_agg)

agg = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.9, 0.1)
  holdings = weights * totalwealth
  wealthtracker = rep(0, n_days) 
  for(today in 1:n_days) {
    return.today = resample(myreturns_agg, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

head(agg)
hist(agg[,n_days], 25, main = "Expected Return")
mean_agg = mean(agg[,n_days])
abline(v=mean_agg, lwd=4, col='blue')

# Profit/loss
hist(agg[,n_days]- 10000, main = "Expected Profit/Loss")

# Calculate 5% value at risk
aggVaR = quantile(agg[,n_days], 0.05) - 10000


```

The even split portfolio's expected return (mean) on the 20th trading day is $`r as.integer(mean_even_split)` which produces a profit of $`r as.integer(mean_even_split - 100000)` and the 5% Value at Risk is $`r as.integer(evenVaR)`, that is, a loss of $`r as.integer(100000 - evenVaR)`.
The safe portfolio's expected return (mean) is $`r as.integer(mean_safe)` which produces a profit of $`r as.integer(mean_safe - 100000)` and the 5% Value at Risk is $`r as.integer(safeVaR)`, that is, a loss of $`r as.integer(100000 - safeVaR)`.
The aggressive portfolio's expected return (mean) is $`r as.integer(mean_agg)` which produces a profit of $`r as.integer(mean_agg - 100000)` and the 5% Value at Risk is $`r as.integer(aggVaR)`, that is, a loss of $`r as.integer(100000 - aggVaR)`.

If the "safer" portfolio is chosen, less profit would be made, but less risk undertook. If the more aggressive portfolio is chosen, surprisingly, a lower return than that from the even split (but higher than the "safer" choice) is expected; yet, there would be a higher risk of loss. Investors can make intelligent decisions using the results above to choose which of the three options to invest in. 

#### Conclusions: 
Based on the analysis, a person who is reluctant to take on a risk should choose the safer choice. In contrast, a person who are willing to take higher risks to achieve above-average returns should go with more aggressive portfolio in theory. The person making this type of decision should weigh all the factors involved in the risk and assess these risks against the probabilities of different outcomes. Unfortunately, the results produced show the aggressive option gives a lower expected return than the even split; therefore, it is not worth the risk. A risk-neutral individual will choose the assets with the highest possible gains or returns without taking into account possible outcomes, so the even split would be the optimal choice in this case.
 

### Marketing

#### Overview: 
We looked to categorize followers of NutrientH2O on Twitter to help NutrientH2O understand their customers, particularly their primary interests (as shown by the topics they tend tweet about). This would help NutrientH2O with targeted marketing and their ad campaigns focus on their primary audience and make them more aware of the types of customer they're lacking in and can expand to. 

#### Data and Models: 
This dataset contained the categorized tweets of users who follow NutrientH2O on Twitter. We expect a little noise in this data since they were categorized by Amazon Mechanical Turk users who are not infallible (and often are bots themselves). Because we have a wide variety of categories, and we really do not understand what types of groups we're going to find, we used PCA, an unsupervised learning method. 


```{r echo = FALSE, warning=FALSE}

userdata = read.csv("C:/Users/hilla/Desktop/R Dir/Stats part 2/social_marketing.csv", header=TRUE, row.names=1)

# Normalize phrase counts to phrase frequencies
Z = userdata/rowSums(userdata)

library(ggplot2)
library(wordcloud)
library(RColorBrewer)

# PCA
pc2 = prcomp(Z, scale=TRUE, center = TRUE)
loadings = pc2$rotation
scores = pc2$x

summary(pc2)

wordcloud(colnames(userdata), colSums(userdata), min.freq=0, max.words = 100) 

qplot(scores[,1], scores[,2], xlab = 'Component 1', ylab = 'Component 2', main = "Figure 1")
### High PC2 and low PC1 
#"school" "food" "parenting"     "sports_fandom" "religion"  
#"health_nutrition" "personal_fitness" "outdoors" "cooking"          "fashion"

qplot(scores[,4], scores[,2], xlab = "Component 4", ylab = "Component 2", main = "Figure 2")

### High PC4 and High PC2 
#"news"          "politics"      "chatter"       "shopping"      "photo_sharing"
#"health_nutrition" "personal_fitness" "outdoors"         "cooking"          "fashion" 

qplot(scores[,5], scores[,4], xlab = "Component 5", ylab = "Component 4", main = "Figure 3")
### Low PC5 high PC4 
# "news"     "politics" "cooking"  "beauty"   "fashion" 
#"college_uni"    "online_gaming"  "sports_playing" "tv_film" "art" 
```

```{r echo= FALSE}
### The top words associated with each component
o1 = order(loadings[,1])
#colnames(Z)[head(o1,5)]
colnames(Z)[tail(o1,5)]

o2 = order(loadings[,2])
colnames(Z)[head(o2,5)]
#colnames(Z)[tail(o2,5)]

#o3 = order(loadings[,3])
#colnames(Z)[head(o3,5)]
#colnames(Z)[tail(o3,5)]

#o4 = order(loadings[,4])
#colnames(Z)[head(o4,5)]
#colnames(Z)[tail(o4,5)]

#o5 = order(loadings[,5])
#colnames(Z)[head(o5,5)]
#colnames(Z)[tail(o5,5)]

#o6 = order(loadings[,6])
#colnames(Z)[head(o6,6)]
#colnames(Z)[tail(o6,6)]

#o7 = order(loadings[,7])
#colnames(Z)[head(o7,7)]
#colnames(Z)[tail(o7,7)]

#o8 = order(loadings[,8])
#colnames(Z)[head(o8,8)]
#colnames(Z)[tail(o8,8)]

#o9 = order(loadings[,9])
#colnames(Z)[head(o9,9)]
#colnames(Z)[tail(o9,9)]

```

#### Results:
Using PCA, 33 principal components were identified. Each component accounted for less than 9% of the variation, so this is clearly a very varied group of individuals. **Figure 1, 2, and 3** show several plots of the components. The highly positive and negative variable weights in each component are shown, which is used as the way of identifying the topics the customers were Tweeting about. For example, in **Figure 1**, there is a concentration of customers at positive PC2 and negative PC1. These points were associated with these variables: school, food, parenting, sports fandom, and religion as well as health nutrition, personal fitness, outdoors, cooking, and fashion.

#### Conclusion: 
The followers of NutrientH2O are extremely varied, covering most of the ranges of all of our principal component models. While NutrientH2O has a very wide base, there seemed to be strong clusters of followers who tweeted about food, sports, fitness, religion, and the outdoors (using principal components 1 and 2). This demographic looks strongly like the health conscious Millennials. This is supported when looking at other principal components (2 and 4). This group is similar to the first but has additional interests in video games, playing sports, and university. Not only do these seem to be college students/Millennials, but the presence of religion and sports may point towards the Southeast and Southwest US which are generally more religious.

Looking into the areas without followers that NutrientH2O can expand into, the empty areas consistently seemed to represent potential customers interested in automotives, politics, and the news. These topics tend to interest and older demographic which is intuitive since the primary demographic of NutrientH2O's followers appeared to be younger. 